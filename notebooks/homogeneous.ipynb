{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Idea #1 - comparison of segregation in Boston vs Philly (now and prior)\n",
    "\n",
    "Ways of measuring:\n",
    "\n",
    "*Compare percent of different geographies (from blocks to ZIP code) that are predominately one race or ethnicity\n",
    "\n",
    "*The same but white vs minority in general\n",
    "\n",
    "*The same as either but also considering population of a tract, and distance to close tract with a predominance that is not your own. Classify the tracts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import charts_function_list\n",
    "import os\n",
    "base,data,outputs = charts_function_list.folder_setup()\n",
    "from census import Census\n",
    "from us import states\n",
    "key = key\n",
    "c = Census(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class state_things(object):\n",
    "    def __init__(self,state_name):\n",
    "        \n",
    "        temp_df = pd.DataFrame(c.acs5.state('NAME', Census.ALL))\n",
    "        self.state_name = state_name\n",
    "        self.state_code = temp_df[temp_df['NAME']==self.state_name]['state'].values[0]\n",
    "        self.county_codes_full = pd.DataFrame(c.acs5.get('NAME', geo={'for': 'county:*',\n",
    "                       'in': 'state:'+self.state_code}))\n",
    "        \n",
    "        self.county_codes = {k:v for k,v in zip(self.county_codes_full['county'],self.county_codes_full['NAME'])}\n",
    "        \n",
    "        \n",
    "    def search_for_county(self,county_name):\n",
    "        return self.county_codes_full[self.county_codes_full['NAME'].str.contains(county_name)]\n",
    "    \n",
    "    \n",
    "        \n",
    "    def get_by_tract(self,county_code,additional_fields=None):\n",
    "        if additional_fields == None:\n",
    "            tract_df = pd.DataFrame(c.acs5.state_county_tract('NAME', self.state_code,county_code, Census.ALL))\n",
    "            \n",
    "        else:\n",
    "            tract_df = pd.DataFrame(c.acs5.state_county_tract(tuple(['NAME'])+tuple(additional_fields), self.state_code,county_code, Census.ALL))\n",
    "      \n",
    "        tract_df['county_map'] = tract_df['county'].map(self.county_codes)\n",
    "        return tract_df\n",
    "        \n",
    "    def get_places(self,additional_fields=None):\n",
    "        if additional_fields == None:\n",
    "            place_df = pd.DataFrame(c.acs5.state_place('NAME',self.state_code, place=Census.ALL))\n",
    "\n",
    "        else:\n",
    "            place_df = pd.DataFrame(c.acs5.state_place((tuple(['NAME'])+tuple(additional_fields)),\n",
    "                                                    self.state_code, place= Census.ALL))\n",
    "        return place_df\n",
    "\n",
    "#find fields in a particular table and return as a dictionary\n",
    "def fields_in_table(table_number):\n",
    "    table_list =  list(filter(lambda x: table_number in x, c.acs5.fields().keys()))\n",
    "    labels = [c.acs5.fields()[x]['label'] for x in table_list]\n",
    "    return {k:v for k,v in zip(table_list,labels)} \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 705,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "state_codes = c.acs5.state('NAME', Census.ALL)\n",
    "state_code_frame = pd.DataFrame(state_codes)\n",
    "d = {name:state_things(name) for name in state_code_frame['NAME']}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get necessary fields related to Race by Hispanic/Latino Origin table, B03002"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 709,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "us_places = []\n",
    "us_tracts = []\n",
    "\n",
    "#create dictionary to get fields in the Race by Hispanic Origin table\n",
    "hisp_race_dict = fields_in_table(table_number='B03002')\n",
    "\n",
    "#take just the keys to generate the tables\n",
    "fields_we_want = list(hisp_race_dict.keys())\n",
    "\n",
    "for states in list(d.keys()):\n",
    "    #pull all the places for each state\n",
    "    us_places.append(d[states].get_places(additional_fields=fields_we_want))\n",
    "    \n",
    "    for counties in list(d[states].county_codes.keys()):\n",
    "        us_tracts.append(d[states].get_by_tract(county_code=counties,additional_fields=fields_we_want))\n",
    "\n",
    "place_race_frame = pd.concat(us_places).reset_index(drop=True)\n",
    "tract_race_frame = pd.concat(us_tracts).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 963,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def reformat_race(df,geo):\n",
    "    race_mapping = {'B03002_001E':'TOTAL',\n",
    "                       'B03002_003E':'WHITE', #White = White NH\n",
    "                       'B03002_004E':'BLACK', #Black = Black NH\n",
    "                       'B03002_005E':'AK_NH',\n",
    "                       'B03002_006E':'ASIAN_NH',\n",
    "                       'B03002_007E':'PI_NH',\n",
    "                       'B03002_008E':'OTHER_NH',\n",
    "                       'B03002_009E':'MIX_NH',\n",
    "                       'B03002_012E':'HISP_ALL',\n",
    "                       'B03002_013E':'WHITE_H',\n",
    "                       'B03002_014E':'BLACK_H',\n",
    "                       'B03002_015E':'AK_H',\n",
    "                       'B03002_016E':'ASIAN_H',\n",
    "                       'B03002_017E':'PI_H',\n",
    "                       'B03002_018E':'OTHER_H',\n",
    "                       'B03002_019E':'MIX_H'}\n",
    "    df = df.rename(columns=race_mapping)\n",
    "    if geo == 'place':\n",
    "        df = df[list(race_mapping.values())+['NAME', 'place', 'state']]\n",
    "    elif geo == 'tract':\n",
    "         df = df[list(race_mapping.values())+['NAME', 'tract', 'state','county','county_map']]\n",
    "    \n",
    "    df['AK'] = df['AK_NH']+df['AK_H']\n",
    "    df['API']= df['ASIAN_H']+df['ASIAN_NH']+df['PI_H']+df['PI_NH']\n",
    "    df['OTHER_MIX']=df['OTHER_NH']+df['MIX_NH']\n",
    "    df['HISP'] = df['HISP_ALL']-df['AK_H']-df['PI_H']-df['ASIAN_H']\n",
    "    def percent_group(column):\n",
    "        return df[column]/df['TOTAL']\n",
    "    \n",
    "    percent_frame = pd.DataFrame({'PER_WHITE':percent_group('WHITE'),\n",
    "                                 'PER_BLACK':percent_group('BLACK'),\n",
    "                                 'PER_API':percent_group('API'),\n",
    "                                 'PER_AKNA':percent_group('AK'),\n",
    "                                  'PER_HISP':percent_group('HISP'),\n",
    "                                  'PER_OTHER':percent_group('OTHER_MIX')\n",
    "                                 \n",
    "                                 })\n",
    "    \n",
    "    df = pd.merge(df,percent_frame,left_index=True,right_index=True)\n",
    "    df['MAX_ETH_PER']= percent_frame.max(axis=1)\n",
    "    df['MAX_ETH']= percent_frame.idxmax(axis=1)\n",
    "    \n",
    "    def threshold_percent(threshold):\n",
    "         return np.where(df['MAX_ETH_PER']>=threshold,1,0)\n",
    "    \n",
    "    df['Above 70'],df['Above 80'],df['Above 90']=threshold_percent(.7),threshold_percent(.8),threshold_percent(.9)\n",
    "    \n",
    "    return df\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 964,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "place_race_frame_full = reformat_race(place_race_frame,geo='place').sort_values(by='MAX_ETH_PER',ascending=False)\n",
    "tract_race_frame_full = reformat_race(tract_race_frame,geo='tract').sort_values(by='MAX_ETH_PER',ascending=False)\n",
    "tract_race_frame_full['GEO_ID']=tract_race_frame_full[['state','county','tract']].astype('str').apply(lambda x: x.sum(),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1008,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "os.chdir(data)\n",
    "place_race_frame_full.to_csv('2016_ACS_race_place_full.csv')\n",
    "tract_race_frame_full.to_csv('2016_ACS_race_tract_full.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1012,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TOTAL</th>\n",
       "      <th>WHITE</th>\n",
       "      <th>BLACK</th>\n",
       "      <th>AK_NH</th>\n",
       "      <th>ASIAN_NH</th>\n",
       "      <th>PI_NH</th>\n",
       "      <th>OTHER_NH</th>\n",
       "      <th>MIX_NH</th>\n",
       "      <th>HISP_ALL</th>\n",
       "      <th>WHITE_H</th>\n",
       "      <th>BLACK_H</th>\n",
       "      <th>AK_H</th>\n",
       "      <th>ASIAN_H</th>\n",
       "      <th>PI_H</th>\n",
       "      <th>OTHER_H</th>\n",
       "      <th>MIX_H</th>\n",
       "      <th>AK</th>\n",
       "      <th>API</th>\n",
       "      <th>OTHER_MIX</th>\n",
       "      <th>HISP</th>\n",
       "      <th>PER_AKNA</th>\n",
       "      <th>PER_API</th>\n",
       "      <th>PER_BLACK</th>\n",
       "      <th>PER_HISP</th>\n",
       "      <th>PER_OTHER</th>\n",
       "      <th>PER_WHITE</th>\n",
       "      <th>MAX_ETH_PER</th>\n",
       "      <th>Above 70</th>\n",
       "      <th>Above 80</th>\n",
       "      <th>Above 90</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>74001.000000</td>\n",
       "      <td>74001.000000</td>\n",
       "      <td>74001.000000</td>\n",
       "      <td>74001.000000</td>\n",
       "      <td>74001.000000</td>\n",
       "      <td>74001.000000</td>\n",
       "      <td>74001.000000</td>\n",
       "      <td>74001.000000</td>\n",
       "      <td>74001.000000</td>\n",
       "      <td>74001.000000</td>\n",
       "      <td>74001.000000</td>\n",
       "      <td>74001.000000</td>\n",
       "      <td>74001.000000</td>\n",
       "      <td>74001.000000</td>\n",
       "      <td>74001.000000</td>\n",
       "      <td>74001.000000</td>\n",
       "      <td>74001.000000</td>\n",
       "      <td>74001.000000</td>\n",
       "      <td>74001.000000</td>\n",
       "      <td>74001.000000</td>\n",
       "      <td>73306.000000</td>\n",
       "      <td>73306.000000</td>\n",
       "      <td>73306.000000</td>\n",
       "      <td>73306.000000</td>\n",
       "      <td>73306.000000</td>\n",
       "      <td>73306.000000</td>\n",
       "      <td>73306.000000</td>\n",
       "      <td>74001.000000</td>\n",
       "      <td>74001.000000</td>\n",
       "      <td>74001.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>4352.475602</td>\n",
       "      <td>2667.386549</td>\n",
       "      <td>528.386441</td>\n",
       "      <td>28.167403</td>\n",
       "      <td>221.992784</td>\n",
       "      <td>6.878421</td>\n",
       "      <td>9.158255</td>\n",
       "      <td>97.391157</td>\n",
       "      <td>793.114593</td>\n",
       "      <td>523.180646</td>\n",
       "      <td>19.740004</td>\n",
       "      <td>7.097404</td>\n",
       "      <td>2.655518</td>\n",
       "      <td>0.691788</td>\n",
       "      <td>201.767746</td>\n",
       "      <td>37.981487</td>\n",
       "      <td>35.264807</td>\n",
       "      <td>232.218511</td>\n",
       "      <td>106.549411</td>\n",
       "      <td>782.669883</td>\n",
       "      <td>0.008880</td>\n",
       "      <td>0.048667</td>\n",
       "      <td>0.132879</td>\n",
       "      <td>0.168072</td>\n",
       "      <td>0.024345</td>\n",
       "      <td>0.617157</td>\n",
       "      <td>0.746134</td>\n",
       "      <td>0.614816</td>\n",
       "      <td>0.456359</td>\n",
       "      <td>0.256497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>2175.316924</td>\n",
       "      <td>1889.675151</td>\n",
       "      <td>902.984218</td>\n",
       "      <td>178.352648</td>\n",
       "      <td>486.204507</td>\n",
       "      <td>50.717061</td>\n",
       "      <td>33.412553</td>\n",
       "      <td>123.721987</td>\n",
       "      <td>1264.186601</td>\n",
       "      <td>930.526358</td>\n",
       "      <td>75.115015</td>\n",
       "      <td>25.951491</td>\n",
       "      <td>12.788123</td>\n",
       "      <td>8.874696</td>\n",
       "      <td>434.157866</td>\n",
       "      <td>90.022092</td>\n",
       "      <td>183.869191</td>\n",
       "      <td>503.464694</td>\n",
       "      <td>131.112594</td>\n",
       "      <td>1253.893232</td>\n",
       "      <td>0.046403</td>\n",
       "      <td>0.091730</td>\n",
       "      <td>0.217001</td>\n",
       "      <td>0.228248</td>\n",
       "      <td>0.026013</td>\n",
       "      <td>0.306745</td>\n",
       "      <td>0.177724</td>\n",
       "      <td>0.486642</td>\n",
       "      <td>0.498095</td>\n",
       "      <td>0.436702</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.231494</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>2893.000000</td>\n",
       "      <td>1260.000000</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>23.000000</td>\n",
       "      <td>86.000000</td>\n",
       "      <td>49.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>26.000000</td>\n",
       "      <td>84.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002585</td>\n",
       "      <td>0.007570</td>\n",
       "      <td>0.024418</td>\n",
       "      <td>0.007686</td>\n",
       "      <td>0.386574</td>\n",
       "      <td>0.606651</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>4080.000000</td>\n",
       "      <td>2482.000000</td>\n",
       "      <td>149.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>54.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>63.000000</td>\n",
       "      <td>283.000000</td>\n",
       "      <td>175.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>37.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>59.000000</td>\n",
       "      <td>70.000000</td>\n",
       "      <td>276.000000</td>\n",
       "      <td>0.000778</td>\n",
       "      <td>0.015626</td>\n",
       "      <td>0.037615</td>\n",
       "      <td>0.070142</td>\n",
       "      <td>0.018087</td>\n",
       "      <td>0.709287</td>\n",
       "      <td>0.777951</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>5465.000000</td>\n",
       "      <td>3777.000000</td>\n",
       "      <td>606.000000</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>216.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>131.000000</td>\n",
       "      <td>883.000000</td>\n",
       "      <td>552.000000</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>188.000000</td>\n",
       "      <td>42.000000</td>\n",
       "      <td>24.000000</td>\n",
       "      <td>228.000000</td>\n",
       "      <td>143.000000</td>\n",
       "      <td>869.000000</td>\n",
       "      <td>0.006017</td>\n",
       "      <td>0.051446</td>\n",
       "      <td>0.144482</td>\n",
       "      <td>0.204618</td>\n",
       "      <td>0.033377</td>\n",
       "      <td>0.881449</td>\n",
       "      <td>0.904068</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>61133.000000</td>\n",
       "      <td>37457.000000</td>\n",
       "      <td>18513.000000</td>\n",
       "      <td>9607.000000</td>\n",
       "      <td>12452.000000</td>\n",
       "      <td>3520.000000</td>\n",
       "      <td>1149.000000</td>\n",
       "      <td>3516.000000</td>\n",
       "      <td>27381.000000</td>\n",
       "      <td>23937.000000</td>\n",
       "      <td>2998.000000</td>\n",
       "      <td>674.000000</td>\n",
       "      <td>819.000000</td>\n",
       "      <td>921.000000</td>\n",
       "      <td>8676.000000</td>\n",
       "      <td>3281.000000</td>\n",
       "      <td>9655.000000</td>\n",
       "      <td>12452.000000</td>\n",
       "      <td>3516.000000</td>\n",
       "      <td>27360.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.911438</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              TOTAL         WHITE         BLACK         AK_NH      ASIAN_NH  \\\n",
       "count  74001.000000  74001.000000  74001.000000  74001.000000  74001.000000   \n",
       "mean    4352.475602   2667.386549    528.386441     28.167403    221.992784   \n",
       "std     2175.316924   1889.675151    902.984218    178.352648    486.204507   \n",
       "min        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "25%     2893.000000   1260.000000     27.000000      0.000000      6.000000   \n",
       "50%     4080.000000   2482.000000    149.000000      0.000000     54.000000   \n",
       "75%     5465.000000   3777.000000    606.000000     15.000000    216.000000   \n",
       "max    61133.000000  37457.000000  18513.000000   9607.000000  12452.000000   \n",
       "\n",
       "              PI_NH      OTHER_NH        MIX_NH      HISP_ALL       WHITE_H  \\\n",
       "count  74001.000000  74001.000000  74001.000000  74001.000000  74001.000000   \n",
       "mean       6.878421      9.158255     97.391157    793.114593    523.180646   \n",
       "std       50.717061     33.412553    123.721987   1264.186601    930.526358   \n",
       "min        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "25%        0.000000      0.000000     23.000000     86.000000     49.000000   \n",
       "50%        0.000000      0.000000     63.000000    283.000000    175.000000   \n",
       "75%        0.000000      1.000000    131.000000    883.000000    552.000000   \n",
       "max     3520.000000   1149.000000   3516.000000  27381.000000  23937.000000   \n",
       "\n",
       "            BLACK_H          AK_H       ASIAN_H          PI_H       OTHER_H  \\\n",
       "count  74001.000000  74001.000000  74001.000000  74001.000000  74001.000000   \n",
       "mean      19.740004      7.097404      2.655518      0.691788    201.767746   \n",
       "std       75.115015     25.951491     12.788123      8.874696    434.157866   \n",
       "min        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "25%        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "50%        0.000000      0.000000      0.000000      0.000000     37.000000   \n",
       "75%       11.000000      0.000000      0.000000      0.000000    188.000000   \n",
       "max     2998.000000    674.000000    819.000000    921.000000   8676.000000   \n",
       "\n",
       "              MIX_H            AK           API     OTHER_MIX          HISP  \\\n",
       "count  74001.000000  74001.000000  74001.000000  74001.000000  74001.000000   \n",
       "mean      37.981487     35.264807    232.218511    106.549411    782.669883   \n",
       "std       90.022092    183.869191    503.464694    131.112594   1253.893232   \n",
       "min        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "25%        0.000000      0.000000      8.000000     26.000000     84.000000   \n",
       "50%        7.000000      2.000000     59.000000     70.000000    276.000000   \n",
       "75%       42.000000     24.000000    228.000000    143.000000    869.000000   \n",
       "max     3281.000000   9655.000000  12452.000000   3516.000000  27360.000000   \n",
       "\n",
       "           PER_AKNA       PER_API     PER_BLACK      PER_HISP     PER_OTHER  \\\n",
       "count  73306.000000  73306.000000  73306.000000  73306.000000  73306.000000   \n",
       "mean       0.008880      0.048667      0.132879      0.168072      0.024345   \n",
       "std        0.046403      0.091730      0.217001      0.228248      0.026013   \n",
       "min        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "25%        0.000000      0.002585      0.007570      0.024418      0.007686   \n",
       "50%        0.000778      0.015626      0.037615      0.070142      0.018087   \n",
       "75%        0.006017      0.051446      0.144482      0.204618      0.033377   \n",
       "max        1.000000      0.911438      1.000000      1.000000      1.000000   \n",
       "\n",
       "          PER_WHITE   MAX_ETH_PER      Above 70      Above 80      Above 90  \n",
       "count  73306.000000  73306.000000  74001.000000  74001.000000  74001.000000  \n",
       "mean       0.617157      0.746134      0.614816      0.456359      0.256497  \n",
       "std        0.306745      0.177724      0.486642      0.498095      0.436702  \n",
       "min        0.000000      0.231494      0.000000      0.000000      0.000000  \n",
       "25%        0.386574      0.606651      0.000000      0.000000      0.000000  \n",
       "50%        0.709287      0.777951      1.000000      0.000000      0.000000  \n",
       "75%        0.881449      0.904068      1.000000      1.000000      1.000000  \n",
       "max        1.000000      1.000000      1.000000      1.000000      1.000000  "
      ]
     },
     "execution_count": 1012,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tract_race_frame_full.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selecting what cities to feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "os.chdir(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MOST PER_WHITE: Portland city, Oregon 71.55%\n",
      "MOST PER_BLACK: Detroit city, Michigan 79.40%\n",
      "MOST PER_HISP: El Paso city, Texas 79.76%\n",
      "MOST PER_API: San Jose city, California 34.54%\n",
      "---\n",
      "No place over 500,000K with majorirty AK/Native American or Other/Mixed Race. Finding the max perctages instead\n",
      "---\n",
      "PER_AK:  Albuquerque city, New Mexico\n",
      "PER_OTHER:  Seattle city, Washington\n"
     ]
    }
   ],
   "source": [
    "place_race_frame_full = pd.read_csv('2016_ACS_race_place_full.csv')\n",
    "ak_max = place_race_frame_full[(place_race_frame_full['TOTAL']>500000)]['PER_AKNA'].max()\n",
    "per_other_max = place_race_frame_full[(place_race_frame_full['TOTAL']>500000)]['PER_OTHER'].max()\n",
    "\n",
    "\n",
    "def max_print(group, minimum):\n",
    "    location = place_race_frame_full[(place_race_frame_full['MAX_ETH']==group)&(place_race_frame_full['TOTAL']>minimum)].reset_index().loc[0]['NAME']\n",
    "    value = place_race_frame_full[(place_race_frame_full['MAX_ETH']==group)&(place_race_frame_full['TOTAL']>minimum)].reset_index().loc[0]['MAX_ETH_PER']\n",
    "    print('MOST '+group+\": \"+location+\" \"+format(value*100,'.2f')+'%')\n",
    "for item in ['PER_WHITE','PER_BLACK','PER_HISP','PER_API']:\n",
    "    max_print(item,500000)\n",
    "    \n",
    "print(\"---\")    \n",
    "print(\"No place over 500,000K with majorirty AK/Native American or Other/Mixed Race. Finding the max perctages instead\")\n",
    "print(\"---\")\n",
    "print('PER_AK: ',place_race_frame_full[place_race_frame_full['PER_AKNA']==ak_max].reset_index().loc[0]['NAME'])\n",
    "print('PER_OTHER: ',place_race_frame_full[place_race_frame_full['PER_OTHER']==per_other_max].reset_index().loc[0]['NAME'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>TOTAL</th>\n",
       "      <th>WHITE</th>\n",
       "      <th>BLACK</th>\n",
       "      <th>AK_NH</th>\n",
       "      <th>ASIAN_NH</th>\n",
       "      <th>PI_NH</th>\n",
       "      <th>OTHER_NH</th>\n",
       "      <th>MIX_NH</th>\n",
       "      <th>HISP_ALL</th>\n",
       "      <th>WHITE_H</th>\n",
       "      <th>BLACK_H</th>\n",
       "      <th>AK_H</th>\n",
       "      <th>ASIAN_H</th>\n",
       "      <th>PI_H</th>\n",
       "      <th>OTHER_H</th>\n",
       "      <th>MIX_H</th>\n",
       "      <th>NAME</th>\n",
       "      <th>place</th>\n",
       "      <th>state</th>\n",
       "      <th>AK</th>\n",
       "      <th>API</th>\n",
       "      <th>OTHER_MIX</th>\n",
       "      <th>HISP</th>\n",
       "      <th>PER_AKNA</th>\n",
       "      <th>PER_API</th>\n",
       "      <th>PER_BLACK</th>\n",
       "      <th>PER_HISP</th>\n",
       "      <th>PER_OTHER</th>\n",
       "      <th>PER_WHITE</th>\n",
       "      <th>MAX_ETH_PER</th>\n",
       "      <th>MAX_ETH</th>\n",
       "      <th>Above 70</th>\n",
       "      <th>Above 80</th>\n",
       "      <th>Above 90</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>20125</th>\n",
       "      <td>24973</td>\n",
       "      <td>678058.0</td>\n",
       "      <td>94654.0</td>\n",
       "      <td>22609.0</td>\n",
       "      <td>1594.0</td>\n",
       "      <td>7984.0</td>\n",
       "      <td>991.0</td>\n",
       "      <td>629.0</td>\n",
       "      <td>5576.0</td>\n",
       "      <td>544021.0</td>\n",
       "      <td>472521.0</td>\n",
       "      <td>3313.0</td>\n",
       "      <td>2412.0</td>\n",
       "      <td>590.0</td>\n",
       "      <td>182.0</td>\n",
       "      <td>55500.0</td>\n",
       "      <td>9503.0</td>\n",
       "      <td>El Paso city, Texas</td>\n",
       "      <td>24000</td>\n",
       "      <td>48</td>\n",
       "      <td>4006.0</td>\n",
       "      <td>9747.0</td>\n",
       "      <td>6205.0</td>\n",
       "      <td>540837.0</td>\n",
       "      <td>0.005908</td>\n",
       "      <td>0.014375</td>\n",
       "      <td>0.033344</td>\n",
       "      <td>0.797626</td>\n",
       "      <td>0.009151</td>\n",
       "      <td>0.139596</td>\n",
       "      <td>0.797626</td>\n",
       "      <td>PER_HISP</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Unnamed: 0     TOTAL    WHITE    BLACK   AK_NH  ASIAN_NH  PI_NH  \\\n",
       "20125       24973  678058.0  94654.0  22609.0  1594.0    7984.0  991.0   \n",
       "\n",
       "       OTHER_NH  MIX_NH  HISP_ALL   WHITE_H  BLACK_H    AK_H  ASIAN_H   PI_H  \\\n",
       "20125     629.0  5576.0  544021.0  472521.0   3313.0  2412.0    590.0  182.0   \n",
       "\n",
       "       OTHER_H   MIX_H                 NAME  place  state      AK     API  \\\n",
       "20125  55500.0  9503.0  El Paso city, Texas  24000     48  4006.0  9747.0   \n",
       "\n",
       "       OTHER_MIX      HISP  PER_AKNA   PER_API  PER_BLACK  PER_HISP  \\\n",
       "20125     6205.0  540837.0  0.005908  0.014375   0.033344  0.797626   \n",
       "\n",
       "       PER_OTHER  PER_WHITE  MAX_ETH_PER   MAX_ETH  Above 70  Above 80  \\\n",
       "20125   0.009151   0.139596     0.797626  PER_HISP         1         0   \n",
       "\n",
       "       Above 90  \n",
       "20125         0  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "place_race_frame_full[place_race_frame_full['NAME']=='El Paso city, Texas']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reload_tract = pd.read_csv('2016_ACS_race_tract_full.csv',index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TOTAL</th>\n",
       "      <th>WHITE</th>\n",
       "      <th>BLACK</th>\n",
       "      <th>AK_NH</th>\n",
       "      <th>ASIAN_NH</th>\n",
       "      <th>PI_NH</th>\n",
       "      <th>OTHER_NH</th>\n",
       "      <th>MIX_NH</th>\n",
       "      <th>HISP_ALL</th>\n",
       "      <th>WHITE_H</th>\n",
       "      <th>BLACK_H</th>\n",
       "      <th>AK_H</th>\n",
       "      <th>ASIAN_H</th>\n",
       "      <th>PI_H</th>\n",
       "      <th>OTHER_H</th>\n",
       "      <th>MIX_H</th>\n",
       "      <th>NAME</th>\n",
       "      <th>tract</th>\n",
       "      <th>state</th>\n",
       "      <th>county</th>\n",
       "      <th>county_map</th>\n",
       "      <th>AK</th>\n",
       "      <th>API</th>\n",
       "      <th>OTHER_MIX</th>\n",
       "      <th>HISP</th>\n",
       "      <th>PER_AKNA</th>\n",
       "      <th>PER_API</th>\n",
       "      <th>PER_BLACK</th>\n",
       "      <th>PER_HISP</th>\n",
       "      <th>PER_OTHER</th>\n",
       "      <th>PER_WHITE</th>\n",
       "      <th>MAX_ETH_PER</th>\n",
       "      <th>MAX_ETH</th>\n",
       "      <th>Above 70</th>\n",
       "      <th>Above 80</th>\n",
       "      <th>Above 90</th>\n",
       "      <th>GEO_ID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>55105</th>\n",
       "      <td>3995.0</td>\n",
       "      <td>1591.0</td>\n",
       "      <td>846.0</td>\n",
       "      <td>45.0</td>\n",
       "      <td>122.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>283.0</td>\n",
       "      <td>1089.0</td>\n",
       "      <td>482.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>77.0</td>\n",
       "      <td>529.0</td>\n",
       "      <td>Census Tract 74, Multnomah County, Oregon</td>\n",
       "      <td>7400</td>\n",
       "      <td>41</td>\n",
       "      <td>51</td>\n",
       "      <td>Multnomah County, Oregon</td>\n",
       "      <td>46.0</td>\n",
       "      <td>141.0</td>\n",
       "      <td>283.0</td>\n",
       "      <td>1088.0</td>\n",
       "      <td>0.011514</td>\n",
       "      <td>0.035294</td>\n",
       "      <td>0.211765</td>\n",
       "      <td>0.27234</td>\n",
       "      <td>0.070839</td>\n",
       "      <td>0.398248</td>\n",
       "      <td>0.398248</td>\n",
       "      <td>PER_WHITE</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>41051007400</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        TOTAL   WHITE  BLACK  AK_NH  ASIAN_NH  PI_NH  OTHER_NH  MIX_NH  \\\n",
       "55105  3995.0  1591.0  846.0   45.0     122.0   19.0       0.0   283.0   \n",
       "\n",
       "       HISP_ALL  WHITE_H  BLACK_H  AK_H  ASIAN_H  PI_H  OTHER_H  MIX_H  \\\n",
       "55105    1089.0    482.0      0.0   1.0      0.0   0.0     77.0  529.0   \n",
       "\n",
       "                                            NAME  tract  state  county  \\\n",
       "55105  Census Tract 74, Multnomah County, Oregon   7400     41      51   \n",
       "\n",
       "                     county_map    AK    API  OTHER_MIX    HISP  PER_AKNA  \\\n",
       "55105  Multnomah County, Oregon  46.0  141.0      283.0  1088.0  0.011514   \n",
       "\n",
       "        PER_API  PER_BLACK  PER_HISP  PER_OTHER  PER_WHITE  MAX_ETH_PER  \\\n",
       "55105  0.035294   0.211765   0.27234   0.070839   0.398248     0.398248   \n",
       "\n",
       "         MAX_ETH  Above 70  Above 80  Above 90       GEO_ID  \n",
       "55105  PER_WHITE         0         0         0  41051007400  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reload_tract[reload_tract['GEO_ID']==41051007400]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 510,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#percent of population that lives in a tract that is >=x% of one ethnic group\n",
    "def homogenous_pop(df,threshold_column):\n",
    "    percent = (df[df[threshold_column]==1]['TOTAL'].sum())/df['TOTAL'].sum()\n",
    "    return format(percent*100,'.1f')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Some extra stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#For those who are non-white and Hispanic, which to include within the \"hispanic\" group? Let's assess\n",
    "\n",
    "for counties in [reformat_race(la_county_data), reformat_race(philly_county_data),reformat_race(suffolk_county_data)]:\n",
    "    for item in ['White_H','Black_H','AK_H','Asian_H','PI_H','OTHER_H','MIX_H']:\n",
    "        print(item+' '+str(format(((counties[item]/counties['HISP_All']).mean())*100,'.1f'))+'%')\n",
    "    print('------')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## extra tutorial stuff\n",
    "\n",
    "c.acs5.get(('NAME', table_name),\n",
    "          {'for': 'state:{}'.format(states.MD.fips)},year=2011) #set different years\n",
    "\n",
    "\n",
    "\n",
    "table_name ='B25034_010E'\n",
    "\n",
    "#The get method is the core data access method on both the ACS and SF1 data sets. \n",
    "#The first parameter is either a single string column or a tuple of columns. \n",
    "#The second parameter is a geoemtry dict with a for key and on option in key. \n",
    "c.acs5.get(('NAME', table_name),\n",
    "          {'for': 'state:{}'.format(states.MD.fips)},year=2011) #set different years"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Some info on Census and US libraries from \n",
    "\n",
    "https://github.com/datamade/census\n",
    "\n",
    "## Description\n",
    "\n",
    "#### ACS5 \n",
    "state(fields, state_fips)\n",
    "\n",
    "\n",
    "state_county(fields, state_fips, county_fips)\n",
    "\n",
    "\n",
    "state_county_blockgroup(fields, state_fips, county_fips, blockgroup)\n",
    "\n",
    "\n",
    "state_county_subdivision(fields, state_fips, county_fips, subdiv_fips)\n",
    "\n",
    "\n",
    "state_county_tract(fields, state_fips, county_fips, tract)\n",
    "\n",
    "\n",
    "state_place(fields, state_fips, place)\n",
    "\n",
    "\n",
    "state_district(fields, state_fips, district)\n",
    "\n",
    "\n",
    "us(fields)\n",
    "\n",
    "zipcode(fields, zip5)\n",
    "\n",
    "#### ACS1\n",
    "\n",
    "ACS1 Geographies\n",
    "\n",
    "state(fields, state_fips)\n",
    "\n",
    "state_district(fields, state_fips, district)\n",
    "us(fields)\n",
    "\n",
    "\n",
    "## Datasets\n",
    "*acs5: ACS 5 Year Estimates (2016, 2015, 2014, 2013, 2012, 2011, 2010)\n",
    "\n",
    "*acs1dp: ACS 1 Year Estimates, Data Profiles (2016, 2015, 2014, 2013, 2012)\n",
    "\n",
    "*sf1: Census Summary File 1 (2010, 2000, 1990)\n",
    "\n",
    "*sf3: Census Summary File 3 (2000, 1990)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
